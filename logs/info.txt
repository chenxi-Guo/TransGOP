[04/26 10:14:43.428]: git:
  sha: 2416279a9e010639b5e1f6922b96454707e62207, status: has uncommited changes, branch: main

[04/26 10:14:43.428]: Command: /data1/gcx003/Transgop_list/TransGOP/main.py -c config/DINO/DINO_4scale.py
[04/26 10:14:43.429]: Full config saved to logs/config_args_all.json
[04/26 10:14:43.429]: world size: 1
[04/26 10:14:43.429]: rank: 0
[04/26 10:14:43.429]: local_rank: 0
[04/26 10:14:43.429]: args: Namespace(add_channel_attention=False, add_pos_value=False, amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=8, bbox_loss_coef=5.0, box_attn_type='roi_align', clip_max_norm=0.1, cls_loss_coef=1.0, coco_panoptic_path=None, coco_path='/data1/gcx003/Datasets/gooreal/gooreal_gazedino/', config_file='config/DINO/DINO_4scale.py', dabdetr_deformable_decoder=False, dabdetr_deformable_encoder=False, dabdetr_yolo_like_anchor_update=False, data_aug_max_size=1333, data_aug_scale_overlap=None, data_aug_scales=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], data_aug_scales2_crop=[384, 600], data_aug_scales2_resize=[400, 500, 600], dataset_file='coco', ddetr_lr_param=False, debug=False, dec_layer_number=None, dec_layers=6, dec_n_points=4, dec_pred_bbox_embed_share=True, dec_pred_class_embed_share=True, decoder_layer_noise=False, decoder_module_seq=['sa', 'ca', 'ffn'], decoder_sa_type='sa', device='cuda', dice_loss_coef=1.0, dilation=False, dim_feedforward=2048, dist_url='env://', distributed=False, dln_hw_noise=0.2, dln_xy_noise=0.2, dn_box_noise_scale=0.4, dn_label_noise_ratio=0.5, dn_labelbook_size=25, dn_number=100, dropout=0.0, ema_decay=0.9997, ema_epoch=0, embed_init_tgt=True, enc_layers=6, enc_loss_coef=1.0, enc_n_points=4, epochs=20, eval=False, find_unused_params=False, finetune_ignore=None, fix_refpoints_hw=-1, fix_size=False, focal_alpha=0.25, frozen_weights=None, gaze_hidden_size=256, giou_loss_coef=2.0, head_backbone='resnet50', hidden_dim=256, interm_loss_coef=1.0, local_rank=0, lr=0.0001, lr_backbone=1e-05, lr_backbone_names=['backbone.0'], lr_drop=11, lr_drop_list=[33, 45], lr_linear_proj_mult=0.1, lr_linear_proj_names=['reference_points', 'sampling_offsets'], mask_loss_coef=1.0, masks=False, match_unstable_error=True, matcher_type='HungarianMatcher', modelname='dino', multi_step_lr=False, nheads=8, nms_iou_threshold=-1, no_interm_box_loss=False, note='', num_classes=25, num_feature_levels=4, num_patterns=0, num_queries=200, num_select=300, num_workers=0, onecyclelr=False, options=None, output_dir='logs/', param_dict_type='default', pdetr3_bbox_embed_diff_each_layer=False, pdetr3_refHW=-1, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, query_dim=4, random_refpoints_xy=False, rank=0, remove_difficult=False, resume='', return_interm_indices=[1, 2, 3], save_checkpoint_interval=1, save_log=False, save_results=False, seed=42, set_cost_bbox=5.0, set_cost_class=2.0, set_cost_giou=2.0, start_epoch=0, test=False, transformer_activation='relu', two_stage_add_query_num=0, two_stage_bbox_embed_share=False, two_stage_class_embed_share=False, two_stage_default_hw=0.05, two_stage_keep_all_tokens=False, two_stage_learn_wh=False, two_stage_pat_embed=0, two_stage_type='standard', unic_layers=0, use_checkpoint=False, use_deformable_box_attn=False, use_detached_boxes_dec_out=False, use_dn=True, use_ema=False, weight_decay=0.0001, world_size=1)

[04/26 10:15:23.802]: git:
  sha: 2416279a9e010639b5e1f6922b96454707e62207, status: has uncommited changes, branch: main

[04/26 10:15:23.802]: Command: /data1/gcx003/Transgop_list/TransGOP/main.py -c config/DINO/DINO_4scale.py
[04/26 10:15:23.829]: Full config saved to logs/config_args_all.json
[04/26 10:15:23.829]: world size: 1
[04/26 10:15:23.829]: rank: 0
[04/26 10:15:23.829]: local_rank: 0
[04/26 10:15:23.829]: args: Namespace(add_channel_attention=False, add_pos_value=False, amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=8, bbox_loss_coef=5.0, box_attn_type='roi_align', clip_max_norm=0.1, cls_loss_coef=1.0, coco_panoptic_path=None, coco_path='/data1/gcx003/Datasets/gooreal/gooreal_gazedino/', config_file='config/DINO/DINO_4scale.py', dabdetr_deformable_decoder=False, dabdetr_deformable_encoder=False, dabdetr_yolo_like_anchor_update=False, data_aug_max_size=1333, data_aug_scale_overlap=None, data_aug_scales=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], data_aug_scales2_crop=[384, 600], data_aug_scales2_resize=[400, 500, 600], dataset_file='coco', ddetr_lr_param=False, debug=False, dec_layer_number=None, dec_layers=6, dec_n_points=4, dec_pred_bbox_embed_share=True, dec_pred_class_embed_share=True, decoder_layer_noise=False, decoder_module_seq=['sa', 'ca', 'ffn'], decoder_sa_type='sa', device='cuda', dice_loss_coef=1.0, dilation=False, dim_feedforward=2048, dist_url='env://', distributed=False, dln_hw_noise=0.2, dln_xy_noise=0.2, dn_box_noise_scale=0.4, dn_label_noise_ratio=0.5, dn_labelbook_size=25, dn_number=100, dropout=0.0, ema_decay=0.9997, ema_epoch=0, embed_init_tgt=True, enc_layers=6, enc_loss_coef=1.0, enc_n_points=4, epochs=20, eval=False, find_unused_params=False, finetune_ignore=None, fix_refpoints_hw=-1, fix_size=False, focal_alpha=0.25, frozen_weights=None, gaze_hidden_size=256, giou_loss_coef=2.0, head_backbone='resnet50', hidden_dim=256, interm_loss_coef=1.0, local_rank=0, lr=0.0001, lr_backbone=1e-05, lr_backbone_names=['backbone.0'], lr_drop=11, lr_drop_list=[33, 45], lr_linear_proj_mult=0.1, lr_linear_proj_names=['reference_points', 'sampling_offsets'], mask_loss_coef=1.0, masks=False, match_unstable_error=True, matcher_type='HungarianMatcher', modelname='dino', multi_step_lr=False, nheads=8, nms_iou_threshold=-1, no_interm_box_loss=False, note='', num_classes=25, num_feature_levels=4, num_patterns=0, num_queries=200, num_select=300, num_workers=0, onecyclelr=False, options=None, output_dir='logs/', param_dict_type='default', pdetr3_bbox_embed_diff_each_layer=False, pdetr3_refHW=-1, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, query_dim=4, random_refpoints_xy=False, rank=0, remove_difficult=False, resume='', return_interm_indices=[1, 2, 3], save_checkpoint_interval=1, save_log=False, save_results=False, seed=42, set_cost_bbox=5.0, set_cost_class=2.0, set_cost_giou=2.0, start_epoch=0, test=False, transformer_activation='relu', two_stage_add_query_num=0, two_stage_bbox_embed_share=False, two_stage_class_embed_share=False, two_stage_default_hw=0.05, two_stage_keep_all_tokens=False, two_stage_learn_wh=False, two_stage_pat_embed=0, two_stage_type='standard', unic_layers=0, use_checkpoint=False, use_deformable_box_attn=False, use_detached_boxes_dec_out=False, use_dn=True, use_ema=False, weight_decay=0.0001, world_size=1)

[04/26 10:15:25.844]: number of params:94285716
[04/26 10:15:25.847]: params:
{
  "transformer.level_embed": 1024,
  "transformer.encoder.layers.0.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.0.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.0.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.0.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.0.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.0.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.0.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.0.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.0.norm1.weight": 256,
  "transformer.encoder.layers.0.norm1.bias": 256,
  "transformer.encoder.layers.0.linear1.weight": 524288,
  "transformer.encoder.layers.0.linear1.bias": 2048,
  "transformer.encoder.layers.0.linear2.weight": 524288,
  "transformer.encoder.layers.0.linear2.bias": 256,
  "transformer.encoder.layers.0.norm2.weight": 256,
  "transformer.encoder.layers.0.norm2.bias": 256,
  "transformer.encoder.layers.1.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.1.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.1.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.1.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.1.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.1.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.1.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.1.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.1.norm1.weight": 256,
  "transformer.encoder.layers.1.norm1.bias": 256,
  "transformer.encoder.layers.1.linear1.weight": 524288,
  "transformer.encoder.layers.1.linear1.bias": 2048,
  "transformer.encoder.layers.1.linear2.weight": 524288,
  "transformer.encoder.layers.1.linear2.bias": 256,
  "transformer.encoder.layers.1.norm2.weight": 256,
  "transformer.encoder.layers.1.norm2.bias": 256,
  "transformer.encoder.layers.2.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.2.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.2.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.2.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.2.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.2.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.2.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.2.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.2.norm1.weight": 256,
  "transformer.encoder.layers.2.norm1.bias": 256,
  "transformer.encoder.layers.2.linear1.weight": 524288,
  "transformer.encoder.layers.2.linear1.bias": 2048,
  "transformer.encoder.layers.2.linear2.weight": 524288,
  "transformer.encoder.layers.2.linear2.bias": 256,
  "transformer.encoder.layers.2.norm2.weight": 256,
  "transformer.encoder.layers.2.norm2.bias": 256,
  "transformer.encoder.layers.3.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.3.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.3.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.3.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.3.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.3.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.3.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.3.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.3.norm1.weight": 256,
  "transformer.encoder.layers.3.norm1.bias": 256,
  "transformer.encoder.layers.3.linear1.weight": 524288,
  "transformer.encoder.layers.3.linear1.bias": 2048,
  "transformer.encoder.layers.3.linear2.weight": 524288,
  "transformer.encoder.layers.3.linear2.bias": 256,
  "transformer.encoder.layers.3.norm2.weight": 256,
  "transformer.encoder.layers.3.norm2.bias": 256,
  "transformer.encoder.layers.4.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.4.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.4.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.4.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.4.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.4.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.4.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.4.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.4.norm1.weight": 256,
  "transformer.encoder.layers.4.norm1.bias": 256,
  "transformer.encoder.layers.4.linear1.weight": 524288,
  "transformer.encoder.layers.4.linear1.bias": 2048,
  "transformer.encoder.layers.4.linear2.weight": 524288,
  "transformer.encoder.layers.4.linear2.bias": 256,
  "transformer.encoder.layers.4.norm2.weight": 256,
  "transformer.encoder.layers.4.norm2.bias": 256,
  "transformer.encoder.layers.5.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.5.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.5.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.5.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.5.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.5.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.5.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.5.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.5.norm1.weight": 256,
  "transformer.encoder.layers.5.norm1.bias": 256,
  "transformer.encoder.layers.5.linear1.weight": 524288,
  "transformer.encoder.layers.5.linear1.bias": 2048,
  "transformer.encoder.layers.5.linear2.weight": 524288,
  "transformer.encoder.layers.5.linear2.bias": 256,
  "transformer.encoder.layers.5.norm2.weight": 256,
  "transformer.encoder.layers.5.norm2.bias": 256,
  "transformer.decoder.layers.0.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.0.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.0.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.0.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.0.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.0.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.0.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.0.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.0.norm1.weight": 256,
  "transformer.decoder.layers.0.norm1.bias": 256,
  "transformer.decoder.layers.0.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.0.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.0.norm2.weight": 256,
  "transformer.decoder.layers.0.norm2.bias": 256,
  "transformer.decoder.layers.0.linear1.weight": 524288,
  "transformer.decoder.layers.0.linear1.bias": 2048,
  "transformer.decoder.layers.0.linear2.weight": 524288,
  "transformer.decoder.layers.0.linear2.bias": 256,
  "transformer.decoder.layers.0.norm3.weight": 256,
  "transformer.decoder.layers.0.norm3.bias": 256,
  "transformer.decoder.layers.1.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.1.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.1.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.1.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.1.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.1.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.1.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.1.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.1.norm1.weight": 256,
  "transformer.decoder.layers.1.norm1.bias": 256,
  "transformer.decoder.layers.1.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.1.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.1.norm2.weight": 256,
  "transformer.decoder.layers.1.norm2.bias": 256,
  "transformer.decoder.layers.1.linear1.weight": 524288,
  "transformer.decoder.layers.1.linear1.bias": 2048,
  "transformer.decoder.layers.1.linear2.weight": 524288,
  "transformer.decoder.layers.1.linear2.bias": 256,
  "transformer.decoder.layers.1.norm3.weight": 256,
  "transformer.decoder.layers.1.norm3.bias": 256,
  "transformer.decoder.layers.2.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.2.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.2.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.2.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.2.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.2.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.2.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.2.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.2.norm1.weight": 256,
  "transformer.decoder.layers.2.norm1.bias": 256,
  "transformer.decoder.layers.2.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.2.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.2.norm2.weight": 256,
  "transformer.decoder.layers.2.norm2.bias": 256,
  "transformer.decoder.layers.2.linear1.weight": 524288,
  "transformer.decoder.layers.2.linear1.bias": 2048,
  "transformer.decoder.layers.2.linear2.weight": 524288,
  "transformer.decoder.layers.2.linear2.bias": 256,
  "transformer.decoder.layers.2.norm3.weight": 256,
  "transformer.decoder.layers.2.norm3.bias": 256,
  "transformer.decoder.layers.3.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.3.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.3.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.3.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.3.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.3.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.3.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.3.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.3.norm1.weight": 256,
  "transformer.decoder.layers.3.norm1.bias": 256,
  "transformer.decoder.layers.3.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.3.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.3.norm2.weight": 256,
  "transformer.decoder.layers.3.norm2.bias": 256,
  "transformer.decoder.layers.3.linear1.weight": 524288,
  "transformer.decoder.layers.3.linear1.bias": 2048,
  "transformer.decoder.layers.3.linear2.weight": 524288,
  "transformer.decoder.layers.3.linear2.bias": 256,
  "transformer.decoder.layers.3.norm3.weight": 256,
  "transformer.decoder.layers.3.norm3.bias": 256,
  "transformer.decoder.layers.4.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.4.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.4.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.4.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.4.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.4.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.4.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.4.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.4.norm1.weight": 256,
  "transformer.decoder.layers.4.norm1.bias": 256,
  "transformer.decoder.layers.4.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.4.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.4.norm2.weight": 256,
  "transformer.decoder.layers.4.norm2.bias": 256,
  "transformer.decoder.layers.4.linear1.weight": 524288,
  "transformer.decoder.layers.4.linear1.bias": 2048,
  "transformer.decoder.layers.4.linear2.weight": 524288,
  "transformer.decoder.layers.4.linear2.bias": 256,
  "transformer.decoder.layers.4.norm3.weight": 256,
  "transformer.decoder.layers.4.norm3.bias": 256,
  "transformer.decoder.layers.5.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.5.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.5.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.5.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.5.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.5.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.5.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.5.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.5.norm1.weight": 256,
  "transformer.decoder.layers.5.norm1.bias": 256,
  "transformer.decoder.layers.5.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.5.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.5.norm2.weight": 256,
  "transformer.decoder.layers.5.norm2.bias": 256,
  "transformer.decoder.layers.5.linear1.weight": 524288,
  "transformer.decoder.layers.5.linear1.bias": 2048,
  "transformer.decoder.layers.5.linear2.weight": 524288,
  "transformer.decoder.layers.5.linear2.bias": 256,
  "transformer.decoder.layers.5.norm3.weight": 256,
  "transformer.decoder.layers.5.norm3.bias": 256,
  "transformer.decoder.norm.weight": 256,
  "transformer.decoder.norm.bias": 256,
  "transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "transformer.decoder.ref_point_head.layers.0.bias": 256,
  "transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "transformer.decoder.ref_point_head.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.0.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.0.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.0.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.0.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.0.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.0.layers.2.bias": 4,
  "transformer.decoder.class_embed.0.weight": 6400,
  "transformer.decoder.class_embed.0.bias": 25,
  "transformer.tgt_embed.weight": 51200,
  "transformer.enc_output.weight": 65536,
  "transformer.enc_output.bias": 256,
  "transformer.enc_output_norm.weight": 256,
  "transformer.enc_output_norm.bias": 256,
  "transformer.enc_out_bbox_embed.layers.0.weight": 65536,
  "transformer.enc_out_bbox_embed.layers.0.bias": 256,
  "transformer.enc_out_bbox_embed.layers.1.weight": 65536,
  "transformer.enc_out_bbox_embed.layers.1.bias": 256,
  "transformer.enc_out_bbox_embed.layers.2.weight": 1024,
  "transformer.enc_out_bbox_embed.layers.2.bias": 4,
  "transformer.enc_out_class_embed.weight": 6400,
  "transformer.enc_out_class_embed.bias": 25,
  "gatector.backbone.model.conv2.weight": 9408,
  "gatector.backbone.model.bn2.weight": 64,
  "gatector.backbone.model.bn2.bias": 64,
  "gatector.backbone.model.layer1.0.conv1.weight": 4096,
  "gatector.backbone.model.layer1.0.bn1.weight": 64,
  "gatector.backbone.model.layer1.0.bn1.bias": 64,
  "gatector.backbone.model.layer1.0.conv2.weight": 36864,
  "gatector.backbone.model.layer1.0.bn2.weight": 64,
  "gatector.backbone.model.layer1.0.bn2.bias": 64,
  "gatector.backbone.model.layer1.0.conv3.weight": 16384,
  "gatector.backbone.model.layer1.0.bn3.weight": 256,
  "gatector.backbone.model.layer1.0.bn3.bias": 256,
  "gatector.backbone.model.layer1.0.downsample.0.weight": 16384,
  "gatector.backbone.model.layer1.0.downsample.1.weight": 256,
  "gatector.backbone.model.layer1.0.downsample.1.bias": 256,
  "gatector.backbone.model.layer1.1.conv1.weight": 16384,
  "gatector.backbone.model.layer1.1.bn1.weight": 64,
  "gatector.backbone.model.layer1.1.bn1.bias": 64,
  "gatector.backbone.model.layer1.1.conv2.weight": 36864,
  "gatector.backbone.model.layer1.1.bn2.weight": 64,
  "gatector.backbone.model.layer1.1.bn2.bias": 64,
  "gatector.backbone.model.layer1.1.conv3.weight": 16384,
  "gatector.backbone.model.layer1.1.bn3.weight": 256,
  "gatector.backbone.model.layer1.1.bn3.bias": 256,
  "gatector.backbone.model.layer1.2.conv1.weight": 16384,
  "gatector.backbone.model.layer1.2.bn1.weight": 64,
  "gatector.backbone.model.layer1.2.bn1.bias": 64,
  "gatector.backbone.model.layer1.2.conv2.weight": 36864,
  "gatector.backbone.model.layer1.2.bn2.weight": 64,
  "gatector.backbone.model.layer1.2.bn2.bias": 64,
  "gatector.backbone.model.layer1.2.conv3.weight": 16384,
  "gatector.backbone.model.layer1.2.bn3.weight": 256,
  "gatector.backbone.model.layer1.2.bn3.bias": 256,
  "gatector.backbone.model.layer2.0.conv1.weight": 32768,
  "gatector.backbone.model.layer2.0.bn1.weight": 128,
  "gatector.backbone.model.layer2.0.bn1.bias": 128,
  "gatector.backbone.model.layer2.0.conv2.weight": 147456,
  "gatector.backbone.model.layer2.0.bn2.weight": 128,
  "gatector.backbone.model.layer2.0.bn2.bias": 128,
  "gatector.backbone.model.layer2.0.conv3.weight": 65536,
  "gatector.backbone.model.layer2.0.bn3.weight": 512,
  "gatector.backbone.model.layer2.0.bn3.bias": 512,
  "gatector.backbone.model.layer2.0.downsample.0.weight": 131072,
  "gatector.backbone.model.layer2.0.downsample.1.weight": 512,
  "gatector.backbone.model.layer2.0.downsample.1.bias": 512,
  "gatector.backbone.model.layer2.1.conv1.weight": 65536,
  "gatector.backbone.model.layer2.1.bn1.weight": 128,
  "gatector.backbone.model.layer2.1.bn1.bias": 128,
  "gatector.backbone.model.layer2.1.conv2.weight": 147456,
  "gatector.backbone.model.layer2.1.bn2.weight": 128,
  "gatector.backbone.model.layer2.1.bn2.bias": 128,
  "gatector.backbone.model.layer2.1.conv3.weight": 65536,
  "gatector.backbone.model.layer2.1.bn3.weight": 512,
  "gatector.backbone.model.layer2.1.bn3.bias": 512,
  "gatector.backbone.model.layer2.2.conv1.weight": 65536,
  "gatector.backbone.model.layer2.2.bn1.weight": 128,
  "gatector.backbone.model.layer2.2.bn1.bias": 128,
  "gatector.backbone.model.layer2.2.conv2.weight": 147456,
  "gatector.backbone.model.layer2.2.bn2.weight": 128,
  "gatector.backbone.model.layer2.2.bn2.bias": 128,
  "gatector.backbone.model.layer2.2.conv3.weight": 65536,
  "gatector.backbone.model.layer2.2.bn3.weight": 512,
  "gatector.backbone.model.layer2.2.bn3.bias": 512,
  "gatector.backbone.model.layer2.3.conv1.weight": 65536,
  "gatector.backbone.model.layer2.3.bn1.weight": 128,
  "gatector.backbone.model.layer2.3.bn1.bias": 128,
  "gatector.backbone.model.layer2.3.conv2.weight": 147456,
  "gatector.backbone.model.layer2.3.bn2.weight": 128,
  "gatector.backbone.model.layer2.3.bn2.bias": 128,
  "gatector.backbone.model.layer2.3.conv3.weight": 65536,
  "gatector.backbone.model.layer2.3.bn3.weight": 512,
  "gatector.backbone.model.layer2.3.bn3.bias": 512,
  "gatector.backbone.model.layer3.0.conv1.weight": 131072,
  "gatector.backbone.model.layer3.0.bn1.weight": 256,
  "gatector.backbone.model.layer3.0.bn1.bias": 256,
  "gatector.backbone.model.layer3.0.conv2.weight": 589824,
  "gatector.backbone.model.layer3.0.bn2.weight": 256,
  "gatector.backbone.model.layer3.0.bn2.bias": 256,
  "gatector.backbone.model.layer3.0.conv3.weight": 262144,
  "gatector.backbone.model.layer3.0.bn3.weight": 1024,
  "gatector.backbone.model.layer3.0.bn3.bias": 1024,
  "gatector.backbone.model.layer3.0.downsample.0.weight": 524288,
  "gatector.backbone.model.layer3.0.downsample.1.weight": 1024,
  "gatector.backbone.model.layer3.0.downsample.1.bias": 1024,
  "gatector.backbone.model.layer3.1.conv1.weight": 262144,
  "gatector.backbone.model.layer3.1.bn1.weight": 256,
  "gatector.backbone.model.layer3.1.bn1.bias": 256,
  "gatector.backbone.model.layer3.1.conv2.weight": 589824,
  "gatector.backbone.model.layer3.1.bn2.weight": 256,
  "gatector.backbone.model.layer3.1.bn2.bias": 256,
  "gatector.backbone.model.layer3.1.conv3.weight": 262144,
  "gatector.backbone.model.layer3.1.bn3.weight": 1024,
  "gatector.backbone.model.layer3.1.bn3.bias": 1024,
  "gatector.backbone.model.layer3.2.conv1.weight": 262144,
  "gatector.backbone.model.layer3.2.bn1.weight": 256,
  "gatector.backbone.model.layer3.2.bn1.bias": 256,
  "gatector.backbone.model.layer3.2.conv2.weight": 589824,
  "gatector.backbone.model.layer3.2.bn2.weight": 256,
  "gatector.backbone.model.layer3.2.bn2.bias": 256,
  "gatector.backbone.model.layer3.2.conv3.weight": 262144,
  "gatector.backbone.model.layer3.2.bn3.weight": 1024,
  "gatector.backbone.model.layer3.2.bn3.bias": 1024,
  "gatector.backbone.model.layer3.3.conv1.weight": 262144,
  "gatector.backbone.model.layer3.3.bn1.weight": 256,
  "gatector.backbone.model.layer3.3.bn1.bias": 256,
  "gatector.backbone.model.layer3.3.conv2.weight": 589824,
  "gatector.backbone.model.layer3.3.bn2.weight": 256,
  "gatector.backbone.model.layer3.3.bn2.bias": 256,
  "gatector.backbone.model.layer3.3.conv3.weight": 262144,
  "gatector.backbone.model.layer3.3.bn3.weight": 1024,
  "gatector.backbone.model.layer3.3.bn3.bias": 1024,
  "gatector.backbone.model.layer3.4.conv1.weight": 262144,
  "gatector.backbone.model.layer3.4.bn1.weight": 256,
  "gatector.backbone.model.layer3.4.bn1.bias": 256,
  "gatector.backbone.model.layer3.4.conv2.weight": 589824,
  "gatector.backbone.model.layer3.4.bn2.weight": 256,
  "gatector.backbone.model.layer3.4.bn2.bias": 256,
  "gatector.backbone.model.layer3.4.conv3.weight": 262144,
  "gatector.backbone.model.layer3.4.bn3.weight": 1024,
  "gatector.backbone.model.layer3.4.bn3.bias": 1024,
  "gatector.backbone.model.layer3.5.conv1.weight": 262144,
  "gatector.backbone.model.layer3.5.bn1.weight": 256,
  "gatector.backbone.model.layer3.5.bn1.bias": 256,
  "gatector.backbone.model.layer3.5.conv2.weight": 589824,
  "gatector.backbone.model.layer3.5.bn2.weight": 256,
  "gatector.backbone.model.layer3.5.bn2.bias": 256,
  "gatector.backbone.model.layer3.5.conv3.weight": 262144,
  "gatector.backbone.model.layer3.5.bn3.weight": 1024,
  "gatector.backbone.model.layer3.5.bn3.bias": 1024,
  "gatector.backbone.model.layer4.0.conv1.weight": 524288,
  "gatector.backbone.model.layer4.0.bn1.weight": 512,
  "gatector.backbone.model.layer4.0.bn1.bias": 512,
  "gatector.backbone.model.layer4.0.conv2.weight": 2359296,
  "gatector.backbone.model.layer4.0.bn2.weight": 512,
  "gatector.backbone.model.layer4.0.bn2.bias": 512,
  "gatector.backbone.model.layer4.0.conv3.weight": 1048576,
  "gatector.backbone.model.layer4.0.bn3.weight": 2048,
  "gatector.backbone.model.layer4.0.bn3.bias": 2048,
  "gatector.backbone.model.layer4.0.downsample.0.weight": 2097152,
  "gatector.backbone.model.layer4.0.downsample.1.weight": 2048,
  "gatector.backbone.model.layer4.0.downsample.1.bias": 2048,
  "gatector.backbone.model.layer4.1.conv1.weight": 1048576,
  "gatector.backbone.model.layer4.1.bn1.weight": 512,
  "gatector.backbone.model.layer4.1.bn1.bias": 512,
  "gatector.backbone.model.layer4.1.conv2.weight": 2359296,
  "gatector.backbone.model.layer4.1.bn2.weight": 512,
  "gatector.backbone.model.layer4.1.bn2.bias": 512,
  "gatector.backbone.model.layer4.1.conv3.weight": 1048576,
  "gatector.backbone.model.layer4.1.bn3.weight": 2048,
  "gatector.backbone.model.layer4.1.bn3.bias": 2048,
  "gatector.backbone.model.layer4.2.conv1.weight": 1048576,
  "gatector.backbone.model.layer4.2.bn1.weight": 512,
  "gatector.backbone.model.layer4.2.bn1.bias": 512,
  "gatector.backbone.model.layer4.2.conv2.weight": 2359296,
  "gatector.backbone.model.layer4.2.bn2.weight": 512,
  "gatector.backbone.model.layer4.2.bn2.bias": 512,
  "gatector.backbone.model.layer4.2.conv3.weight": 1048576,
  "gatector.backbone.model.layer4.2.bn3.weight": 2048,
  "gatector.backbone.model.layer4.2.bn3.bias": 2048,
  "gatector.backbone.model.layer5_scene.0.conv1.weight": 524288,
  "gatector.backbone.model.layer5_scene.0.bn1.weight": 256,
  "gatector.backbone.model.layer5_scene.0.bn1.bias": 256,
  "gatector.backbone.model.layer5_scene.0.conv2.weight": 589824,
  "gatector.backbone.model.layer5_scene.0.bn2.weight": 256,
  "gatector.backbone.model.layer5_scene.0.bn2.bias": 256,
  "gatector.backbone.model.layer5_scene.0.conv3.weight": 262144,
  "gatector.backbone.model.layer5_scene.0.bn3.weight": 1024,
  "gatector.backbone.model.layer5_scene.0.bn3.bias": 1024,
  "gatector.backbone.model.layer5_scene.0.downsample.0.weight": 2097152,
  "gatector.backbone.model.layer5_scene.0.downsample.1.weight": 1024,
  "gatector.backbone.model.layer5_scene.0.downsample.1.bias": 1024,
  "gatector.backbone.model.layer5_scene.1.conv1.weight": 262144,
  "gatector.backbone.model.layer5_scene.1.bn1.weight": 256,
  "gatector.backbone.model.layer5_scene.1.bn1.bias": 256,
  "gatector.backbone.model.layer5_scene.1.conv2.weight": 589824,
  "gatector.backbone.model.layer5_scene.1.bn2.weight": 256,
  "gatector.backbone.model.layer5_scene.1.bn2.bias": 256,
  "gatector.backbone.model.layer5_scene.1.conv3.weight": 262144,
  "gatector.backbone.model.layer5_scene.1.bn3.weight": 1024,
  "gatector.backbone.model.layer5_scene.1.bn3.bias": 1024,
  "gatector.backbone.model.layer5_face.0.conv1.weight": 524288,
  "gatector.backbone.model.layer5_face.0.bn1.weight": 256,
  "gatector.backbone.model.layer5_face.0.bn1.bias": 256,
  "gatector.backbone.model.layer5_face.0.conv2.weight": 589824,
  "gatector.backbone.model.layer5_face.0.bn2.weight": 256,
  "gatector.backbone.model.layer5_face.0.bn2.bias": 256,
  "gatector.backbone.model.layer5_face.0.conv3.weight": 262144,
  "gatector.backbone.model.layer5_face.0.bn3.weight": 1024,
  "gatector.backbone.model.layer5_face.0.bn3.bias": 1024,
  "gatector.backbone.model.layer5_face.0.downsample.0.weight": 2097152,
  "gatector.backbone.model.layer5_face.0.downsample.1.weight": 1024,
  "gatector.backbone.model.layer5_face.0.downsample.1.bias": 1024,
  "gatector.backbone.model.layer5_face.1.conv1.weight": 262144,
  "gatector.backbone.model.layer5_face.1.bn1.weight": 256,
  "gatector.backbone.model.layer5_face.1.bn1.bias": 256,
  "gatector.backbone.model.layer5_face.1.conv2.weight": 589824,
  "gatector.backbone.model.layer5_face.1.bn2.weight": 256,
  "gatector.backbone.model.layer5_face.1.bn2.bias": 256,
  "gatector.backbone.model.layer5_face.1.conv3.weight": 262144,
  "gatector.backbone.model.layer5_face.1.bn3.weight": 1024,
  "gatector.backbone.model.layer5_face.1.bn3.bias": 1024,
  "gatector.conv_face_scene.weight": 5242880,
  "gatector.conv_face_scene.bias": 2048,
  "gatector.conv_trblock.weight": 65536,
  "gatector.conv_trblock.bias": 256,
  "gatector.trblock_bn.weight": 256,
  "gatector.trblock_bn.bias": 256,
  "gatector.conv_block.0.weight": 288,
  "gatector.conv_block.2.weight": 18432,
  "gatector.conv_block.4.weight": 73728,
  "gatector.conv_block.6.weight": 294912,
  "gatector.conv_block.8.weight": 1179648,
  "gatector.attn.weight": 88592,
  "gatector.attn.bias": 49,
  "gatector.compress_conv1.weight": 2097152,
  "gatector.compress_bn1.weight": 1024,
  "gatector.compress_bn1.bias": 1024,
  "gatector.compress_conv2.weight": 524288,
  "gatector.compress_bn2.weight": 512,
  "gatector.compress_bn2.bias": 512,
  "gatector.compress_conv1_inout.weight": 1048576,
  "gatector.compress_bn1_inout.weight": 512,
  "gatector.compress_bn1_inout.bias": 512,
  "gatector.compress_conv2_inout.weight": 512,
  "gatector.compress_bn2_inout.weight": 1,
  "gatector.compress_bn2_inout.bias": 1,
  "gatector.fc_inout.weight": 49,
  "gatector.fc_inout.bias": 1,
  "gatector.deconv1.weight": 1179648,
  "gatector.deconv1.bias": 256,
  "gatector.deconv_bn1.weight": 256,
  "gatector.deconv_bn1.bias": 256,
  "gatector.deconv2.weight": 294912,
  "gatector.deconv2.bias": 128,
  "gatector.deconv_bn2.weight": 128,
  "gatector.deconv_bn2.bias": 128,
  "gatector.deconv3.weight": 2048,
  "gatector.deconv3.bias": 1,
  "gatector.deconv_bn3.weight": 1,
  "gatector.deconv_bn3.bias": 1,
  "gatector.conv4.weight": 1,
  "gatector.conv4.bias": 1,
  "gatector.totrans_conv.weight": 65536,
  "gatector.totrans_conv_bn1.weight": 256,
  "gatector.totrans_conv_bn1.bias": 256,
  "gatector.transformer_layer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "gatector.transformer_layer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "gatector.transformer_layer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "gatector.transformer_layer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "gatector.transformer_layer.encoder.layers.0.linear1.weight": 524288,
  "gatector.transformer_layer.encoder.layers.0.linear1.bias": 2048,
  "gatector.transformer_layer.encoder.layers.0.linear2.weight": 524288,
  "gatector.transformer_layer.encoder.layers.0.linear2.bias": 256,
  "gatector.transformer_layer.encoder.layers.0.norm1.weight": 256,
  "gatector.transformer_layer.encoder.layers.0.norm1.bias": 256,
  "gatector.transformer_layer.encoder.layers.0.norm2.weight": 256,
  "gatector.transformer_layer.encoder.layers.0.norm2.bias": 256,
  "gatector.transformer_layer.decoder.layers.0.self_attn.in_proj_weight": 196608,
  "gatector.transformer_layer.decoder.layers.0.self_attn.in_proj_bias": 768,
  "gatector.transformer_layer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "gatector.transformer_layer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "gatector.transformer_layer.decoder.layers.0.multihead_attn.in_proj_weight": 196608,
  "gatector.transformer_layer.decoder.layers.0.multihead_attn.in_proj_bias": 768,
  "gatector.transformer_layer.decoder.layers.0.multihead_attn.out_proj.weight": 65536,
  "gatector.transformer_layer.decoder.layers.0.multihead_attn.out_proj.bias": 256,
  "gatector.transformer_layer.decoder.layers.0.linear1.weight": 524288,
  "gatector.transformer_layer.decoder.layers.0.linear1.bias": 2048,
  "gatector.transformer_layer.decoder.layers.0.linear2.weight": 524288,
  "gatector.transformer_layer.decoder.layers.0.linear2.bias": 256,
  "gatector.transformer_layer.decoder.layers.0.norm1.weight": 256,
  "gatector.transformer_layer.decoder.layers.0.norm1.bias": 256,
  "gatector.transformer_layer.decoder.layers.0.norm2.weight": 256,
  "gatector.transformer_layer.decoder.layers.0.norm2.bias": 256,
  "gatector.transformer_layer.decoder.layers.0.norm3.weight": 256,
  "gatector.transformer_layer.decoder.layers.0.norm3.bias": 256,
  "gatector.transformer_layer.decoder.layers.0.fc_to512.weight": 65536,
  "gatector.transformer_layer.decoder.layers.0.fc_to512.bias": 256,
  "gatector.transformer_layer.decoder.norm.weight": 256,
  "gatector.transformer_layer.decoder.norm.bias": 256,
  "label_enc.weight": 6656,
  "input_proj.0.0.weight": 131072,
  "input_proj.0.0.bias": 256,
  "input_proj.0.1.weight": 256,
  "input_proj.0.1.bias": 256,
  "input_proj.1.0.weight": 262144,
  "input_proj.1.0.bias": 256,
  "input_proj.1.1.weight": 256,
  "input_proj.1.1.bias": 256,
  "input_proj.2.0.weight": 524288,
  "input_proj.2.0.bias": 256,
  "input_proj.2.1.weight": 256,
  "input_proj.2.1.bias": 256,
  "input_proj.3.0.weight": 4718592,
  "input_proj.3.0.bias": 256,
  "input_proj.3.1.weight": 256,
  "input_proj.3.1.bias": 256,
  "backbone.0.body.layer2.0.conv1.weight": 32768,
  "backbone.0.body.layer2.0.conv2.weight": 147456,
  "backbone.0.body.layer2.0.conv3.weight": 65536,
  "backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "backbone.0.body.layer2.1.conv1.weight": 65536,
  "backbone.0.body.layer2.1.conv2.weight": 147456,
  "backbone.0.body.layer2.1.conv3.weight": 65536,
  "backbone.0.body.layer2.2.conv1.weight": 65536,
  "backbone.0.body.layer2.2.conv2.weight": 147456,
  "backbone.0.body.layer2.2.conv3.weight": 65536,
  "backbone.0.body.layer2.3.conv1.weight": 65536,
  "backbone.0.body.layer2.3.conv2.weight": 147456,
  "backbone.0.body.layer2.3.conv3.weight": 65536,
  "backbone.0.body.layer3.0.conv1.weight": 131072,
  "backbone.0.body.layer3.0.conv2.weight": 589824,
  "backbone.0.body.layer3.0.conv3.weight": 262144,
  "backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "backbone.0.body.layer3.1.conv1.weight": 262144,
  "backbone.0.body.layer3.1.conv2.weight": 589824,
  "backbone.0.body.layer3.1.conv3.weight": 262144,
  "backbone.0.body.layer3.2.conv1.weight": 262144,
  "backbone.0.body.layer3.2.conv2.weight": 589824,
  "backbone.0.body.layer3.2.conv3.weight": 262144,
  "backbone.0.body.layer3.3.conv1.weight": 262144,
  "backbone.0.body.layer3.3.conv2.weight": 589824,
  "backbone.0.body.layer3.3.conv3.weight": 262144,
  "backbone.0.body.layer3.4.conv1.weight": 262144,
  "backbone.0.body.layer3.4.conv2.weight": 589824,
  "backbone.0.body.layer3.4.conv3.weight": 262144,
  "backbone.0.body.layer3.5.conv1.weight": 262144,
  "backbone.0.body.layer3.5.conv2.weight": 589824,
  "backbone.0.body.layer3.5.conv3.weight": 262144,
  "backbone.0.body.layer4.0.conv1.weight": 524288,
  "backbone.0.body.layer4.0.conv2.weight": 2359296,
  "backbone.0.body.layer4.0.conv3.weight": 1048576,
  "backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "backbone.0.body.layer4.1.conv1.weight": 1048576,
  "backbone.0.body.layer4.1.conv2.weight": 2359296,
  "backbone.0.body.layer4.1.conv3.weight": 1048576,
  "backbone.0.body.layer4.2.conv1.weight": 1048576,
  "backbone.0.body.layer4.2.conv2.weight": 2359296,
  "backbone.0.body.layer4.2.conv3.weight": 1048576
}
[04/26 10:22:35.406]: git:
  sha: 2416279a9e010639b5e1f6922b96454707e62207, status: has uncommited changes, branch: main

[04/26 10:22:35.406]: Command: /data1/gcx003/Transgop_list/TransGOP/main.py -c config/TransGOP/TransGOP_4scale.py
[04/26 10:22:35.412]: Full config saved to logs/config_args_all.json
[04/26 10:22:35.412]: world size: 1
[04/26 10:22:35.412]: rank: 0
[04/26 10:22:35.412]: local_rank: 0
[04/26 10:22:35.412]: args: Namespace(add_channel_attention=False, add_pos_value=False, amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=8, bbox_loss_coef=5.0, box_attn_type='roi_align', clip_max_norm=0.1, cls_loss_coef=1.0, coco_panoptic_path=None, coco_path='/data1/gcx003/Datasets/gooreal/gooreal_gazedino/', config_file='config/TransGOP/TransGOP_4scale.py', dabdetr_deformable_decoder=False, dabdetr_deformable_encoder=False, dabdetr_yolo_like_anchor_update=False, data_aug_max_size=1333, data_aug_scale_overlap=None, data_aug_scales=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], data_aug_scales2_crop=[384, 600], data_aug_scales2_resize=[400, 500, 600], dataset_file='coco', ddetr_lr_param=False, debug=False, dec_layer_number=None, dec_layers=6, dec_n_points=4, dec_pred_bbox_embed_share=True, dec_pred_class_embed_share=True, decoder_layer_noise=False, decoder_module_seq=['sa', 'ca', 'ffn'], decoder_sa_type='sa', device='cuda', dice_loss_coef=1.0, dilation=False, dim_feedforward=2048, dist_url='env://', distributed=False, dln_hw_noise=0.2, dln_xy_noise=0.2, dn_box_noise_scale=0.4, dn_label_noise_ratio=0.5, dn_labelbook_size=25, dn_number=100, dropout=0.0, ema_decay=0.9997, ema_epoch=0, embed_init_tgt=True, enc_layers=6, enc_loss_coef=1.0, enc_n_points=4, epochs=20, eval=False, find_unused_params=False, finetune_ignore=None, fix_refpoints_hw=-1, fix_size=False, focal_alpha=0.25, frozen_weights=None, gaze_hidden_size=256, giou_loss_coef=2.0, head_backbone='resnet50', hidden_dim=256, interm_loss_coef=1.0, local_rank=0, lr=0.0001, lr_backbone=1e-05, lr_backbone_names=['backbone.0'], lr_drop=11, lr_drop_list=[33, 45], lr_linear_proj_mult=0.1, lr_linear_proj_names=['reference_points', 'sampling_offsets'], mask_loss_coef=1.0, masks=False, match_unstable_error=True, matcher_type='HungarianMatcher', modelname='TransGOP', multi_step_lr=False, nheads=8, nms_iou_threshold=-1, no_interm_box_loss=False, note='', num_classes=25, num_feature_levels=4, num_patterns=0, num_queries=200, num_select=300, num_workers=0, onecyclelr=False, options=None, output_dir='logs/', param_dict_type='default', pdetr3_bbox_embed_diff_each_layer=False, pdetr3_refHW=-1, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, query_dim=4, random_refpoints_xy=False, rank=0, remove_difficult=False, resume='', return_interm_indices=[1, 2, 3], save_checkpoint_interval=1, save_log=False, save_results=False, seed=42, set_cost_bbox=5.0, set_cost_class=2.0, set_cost_giou=2.0, start_epoch=0, test=False, transformer_activation='relu', two_stage_add_query_num=0, two_stage_bbox_embed_share=False, two_stage_class_embed_share=False, two_stage_default_hw=0.05, two_stage_keep_all_tokens=False, two_stage_learn_wh=False, two_stage_pat_embed=0, two_stage_type='standard', unic_layers=0, use_checkpoint=False, use_deformable_box_attn=False, use_detached_boxes_dec_out=False, use_dn=True, use_ema=False, weight_decay=0.0001, world_size=1)

[04/26 10:24:47.913]: git:
  sha: 2416279a9e010639b5e1f6922b96454707e62207, status: has uncommited changes, branch: main

[04/26 10:24:47.913]: Command: /data1/gcx003/Transgop_list/TransGOP/main.py -c config/TransGOP/TransGOP_4scale.py
[04/26 10:24:47.918]: Full config saved to logs/config_args_all.json
[04/26 10:24:47.919]: world size: 1
[04/26 10:24:47.919]: rank: 0
[04/26 10:24:47.919]: local_rank: 0
[04/26 10:24:47.919]: args: Namespace(add_channel_attention=False, add_pos_value=False, amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=8, bbox_loss_coef=5.0, box_attn_type='roi_align', clip_max_norm=0.1, cls_loss_coef=1.0, coco_panoptic_path=None, coco_path='/data1/gcx003/Datasets/gooreal/gooreal_gazedino/', config_file='config/TransGOP/TransGOP_4scale.py', dabdetr_deformable_decoder=False, dabdetr_deformable_encoder=False, dabdetr_yolo_like_anchor_update=False, data_aug_max_size=1333, data_aug_scale_overlap=None, data_aug_scales=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], data_aug_scales2_crop=[384, 600], data_aug_scales2_resize=[400, 500, 600], dataset_file='coco', ddetr_lr_param=False, debug=False, dec_layer_number=None, dec_layers=6, dec_n_points=4, dec_pred_bbox_embed_share=True, dec_pred_class_embed_share=True, decoder_layer_noise=False, decoder_module_seq=['sa', 'ca', 'ffn'], decoder_sa_type='sa', device='cuda', dice_loss_coef=1.0, dilation=False, dim_feedforward=2048, dist_url='env://', distributed=False, dln_hw_noise=0.2, dln_xy_noise=0.2, dn_box_noise_scale=0.4, dn_label_noise_ratio=0.5, dn_labelbook_size=25, dn_number=100, dropout=0.0, ema_decay=0.9997, ema_epoch=0, embed_init_tgt=True, enc_layers=6, enc_loss_coef=1.0, enc_n_points=4, epochs=20, eval=False, find_unused_params=False, finetune_ignore=None, fix_refpoints_hw=-1, fix_size=False, focal_alpha=0.25, frozen_weights=None, gaze_hidden_size=256, giou_loss_coef=2.0, head_backbone='resnet50', hidden_dim=256, interm_loss_coef=1.0, local_rank=0, lr=0.0001, lr_backbone=1e-05, lr_backbone_names=['backbone.0'], lr_drop=11, lr_drop_list=[33, 45], lr_linear_proj_mult=0.1, lr_linear_proj_names=['reference_points', 'sampling_offsets'], mask_loss_coef=1.0, masks=False, match_unstable_error=True, matcher_type='HungarianMatcher', modelname='TransGOP', multi_step_lr=False, nheads=8, nms_iou_threshold=-1, no_interm_box_loss=False, note='', num_classes=25, num_feature_levels=4, num_patterns=0, num_queries=200, num_select=300, num_workers=0, onecyclelr=False, options=None, output_dir='logs/', param_dict_type='default', pdetr3_bbox_embed_diff_each_layer=False, pdetr3_refHW=-1, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, query_dim=4, random_refpoints_xy=False, rank=0, remove_difficult=False, resume='', return_interm_indices=[1, 2, 3], save_checkpoint_interval=1, save_log=False, save_results=False, seed=42, set_cost_bbox=5.0, set_cost_class=2.0, set_cost_giou=2.0, start_epoch=0, test=False, transformer_activation='relu', two_stage_add_query_num=0, two_stage_bbox_embed_share=False, two_stage_class_embed_share=False, two_stage_default_hw=0.05, two_stage_keep_all_tokens=False, two_stage_learn_wh=False, two_stage_pat_embed=0, two_stage_type='standard', unic_layers=0, use_checkpoint=False, use_deformable_box_attn=False, use_detached_boxes_dec_out=False, use_dn=True, use_ema=False, weight_decay=0.0001, world_size=1)

[04/26 10:25:06.106]: git:
  sha: 2416279a9e010639b5e1f6922b96454707e62207, status: has uncommited changes, branch: main

[04/26 10:25:06.106]: Command: /data1/gcx003/Transgop_list/TransGOP/main.py -c config/TransGOP/TransGOP_4scale.py
[04/26 10:25:06.114]: Full config saved to logs/config_args_all.json
[04/26 10:25:06.114]: world size: 1
[04/26 10:25:06.114]: rank: 0
[04/26 10:25:06.114]: local_rank: 0
[04/26 10:25:06.114]: args: Namespace(add_channel_attention=False, add_pos_value=False, amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=8, bbox_loss_coef=5.0, box_attn_type='roi_align', clip_max_norm=0.1, cls_loss_coef=1.0, coco_panoptic_path=None, coco_path='/data1/gcx003/Datasets/gooreal/gooreal_gazedino/', config_file='config/TransGOP/TransGOP_4scale.py', dabdetr_deformable_decoder=False, dabdetr_deformable_encoder=False, dabdetr_yolo_like_anchor_update=False, data_aug_max_size=1333, data_aug_scale_overlap=None, data_aug_scales=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], data_aug_scales2_crop=[384, 600], data_aug_scales2_resize=[400, 500, 600], dataset_file='coco', ddetr_lr_param=False, debug=False, dec_layer_number=None, dec_layers=6, dec_n_points=4, dec_pred_bbox_embed_share=True, dec_pred_class_embed_share=True, decoder_layer_noise=False, decoder_module_seq=['sa', 'ca', 'ffn'], decoder_sa_type='sa', device='cuda', dice_loss_coef=1.0, dilation=False, dim_feedforward=2048, dist_url='env://', distributed=False, dln_hw_noise=0.2, dln_xy_noise=0.2, dn_box_noise_scale=0.4, dn_label_noise_ratio=0.5, dn_labelbook_size=25, dn_number=100, dropout=0.0, ema_decay=0.9997, ema_epoch=0, embed_init_tgt=True, enc_layers=6, enc_loss_coef=1.0, enc_n_points=4, epochs=20, eval=False, find_unused_params=False, finetune_ignore=None, fix_refpoints_hw=-1, fix_size=False, focal_alpha=0.25, frozen_weights=None, gaze_hidden_size=256, giou_loss_coef=2.0, head_backbone='resnet50', hidden_dim=256, interm_loss_coef=1.0, local_rank=0, lr=0.0001, lr_backbone=1e-05, lr_backbone_names=['backbone.0'], lr_drop=11, lr_drop_list=[33, 45], lr_linear_proj_mult=0.1, lr_linear_proj_names=['reference_points', 'sampling_offsets'], mask_loss_coef=1.0, masks=False, match_unstable_error=True, matcher_type='HungarianMatcher', modelname='TransGOP', multi_step_lr=False, nheads=8, nms_iou_threshold=-1, no_interm_box_loss=False, note='', num_classes=25, num_feature_levels=4, num_patterns=0, num_queries=200, num_select=300, num_workers=0, onecyclelr=False, options=None, output_dir='logs/', param_dict_type='default', pdetr3_bbox_embed_diff_each_layer=False, pdetr3_refHW=-1, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, query_dim=4, random_refpoints_xy=False, rank=0, remove_difficult=False, resume='', return_interm_indices=[1, 2, 3], save_checkpoint_interval=1, save_log=False, save_results=False, seed=42, set_cost_bbox=5.0, set_cost_class=2.0, set_cost_giou=2.0, start_epoch=0, test=False, transformer_activation='relu', two_stage_add_query_num=0, two_stage_bbox_embed_share=False, two_stage_class_embed_share=False, two_stage_default_hw=0.05, two_stage_keep_all_tokens=False, two_stage_learn_wh=False, two_stage_pat_embed=0, two_stage_type='standard', unic_layers=0, use_checkpoint=False, use_deformable_box_attn=False, use_detached_boxes_dec_out=False, use_dn=True, use_ema=False, weight_decay=0.0001, world_size=1)

[04/26 10:26:42.888]: git:
  sha: 2416279a9e010639b5e1f6922b96454707e62207, status: has uncommited changes, branch: main

[04/26 10:26:42.888]: Command: /data1/gcx003/Transgop_list/TransGOP/main.py -c config/TransGOP/TransGOP_4scale.py
[04/26 10:26:42.894]: Full config saved to logs/config_args_all.json
[04/26 10:26:42.894]: world size: 1
[04/26 10:26:42.894]: rank: 0
[04/26 10:26:42.894]: local_rank: 0
[04/26 10:26:42.894]: args: Namespace(add_channel_attention=False, add_pos_value=False, amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=8, bbox_loss_coef=5.0, box_attn_type='roi_align', clip_max_norm=0.1, cls_loss_coef=1.0, coco_panoptic_path=None, coco_path='/data1/gcx003/Datasets/gooreal/gooreal_gazedino/', config_file='config/TransGOP/TransGOP_4scale.py', dabdetr_deformable_decoder=False, dabdetr_deformable_encoder=False, dabdetr_yolo_like_anchor_update=False, data_aug_max_size=1333, data_aug_scale_overlap=None, data_aug_scales=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], data_aug_scales2_crop=[384, 600], data_aug_scales2_resize=[400, 500, 600], dataset_file='coco', ddetr_lr_param=False, debug=False, dec_layer_number=None, dec_layers=6, dec_n_points=4, dec_pred_bbox_embed_share=True, dec_pred_class_embed_share=True, decoder_layer_noise=False, decoder_module_seq=['sa', 'ca', 'ffn'], decoder_sa_type='sa', device='cuda', dice_loss_coef=1.0, dilation=False, dim_feedforward=2048, dist_url='env://', distributed=False, dln_hw_noise=0.2, dln_xy_noise=0.2, dn_box_noise_scale=0.4, dn_label_noise_ratio=0.5, dn_labelbook_size=25, dn_number=100, dropout=0.0, ema_decay=0.9997, ema_epoch=0, embed_init_tgt=True, enc_layers=6, enc_loss_coef=1.0, enc_n_points=4, epochs=20, eval=False, find_unused_params=False, finetune_ignore=None, fix_refpoints_hw=-1, fix_size=False, focal_alpha=0.25, frozen_weights=None, gaze_hidden_size=256, giou_loss_coef=2.0, head_backbone='resnet50', hidden_dim=256, interm_loss_coef=1.0, local_rank=0, lr=0.0001, lr_backbone=1e-05, lr_backbone_names=['backbone.0'], lr_drop=11, lr_drop_list=[33, 45], lr_linear_proj_mult=0.1, lr_linear_proj_names=['reference_points', 'sampling_offsets'], mask_loss_coef=1.0, masks=False, match_unstable_error=True, matcher_type='HungarianMatcher', modelname='TransGOP', multi_step_lr=False, nheads=8, nms_iou_threshold=-1, no_interm_box_loss=False, note='', num_classes=25, num_feature_levels=4, num_patterns=0, num_queries=200, num_select=300, num_workers=0, onecyclelr=False, options=None, output_dir='logs/', param_dict_type='default', pdetr3_bbox_embed_diff_each_layer=False, pdetr3_refHW=-1, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, query_dim=4, random_refpoints_xy=False, rank=0, remove_difficult=False, resume='', return_interm_indices=[1, 2, 3], save_checkpoint_interval=1, save_log=False, save_results=False, seed=42, set_cost_bbox=5.0, set_cost_class=2.0, set_cost_giou=2.0, start_epoch=0, test=False, transformer_activation='relu', two_stage_add_query_num=0, two_stage_bbox_embed_share=False, two_stage_class_embed_share=False, two_stage_default_hw=0.05, two_stage_keep_all_tokens=False, two_stage_learn_wh=False, two_stage_pat_embed=0, two_stage_type='standard', unic_layers=0, use_checkpoint=False, use_deformable_box_attn=False, use_detached_boxes_dec_out=False, use_dn=True, use_ema=False, weight_decay=0.0001, world_size=1)

[04/26 10:27:57.261]: git:
  sha: 2416279a9e010639b5e1f6922b96454707e62207, status: has uncommited changes, branch: main

[04/26 10:27:57.261]: Command: /data1/gcx003/Transgop_list/TransGOP/main.py -c config/TransGOP/TransGOP_4scale.py
[04/26 10:27:57.269]: Full config saved to logs/config_args_all.json
[04/26 10:27:57.269]: world size: 1
[04/26 10:27:57.269]: rank: 0
[04/26 10:27:57.269]: local_rank: 0
[04/26 10:27:57.269]: args: Namespace(add_channel_attention=False, add_pos_value=False, amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=8, bbox_loss_coef=5.0, box_attn_type='roi_align', clip_max_norm=0.1, cls_loss_coef=1.0, coco_panoptic_path=None, coco_path='/data1/gcx003/Datasets/gooreal/gooreal_gazedino/', config_file='config/TransGOP/TransGOP_4scale.py', dabdetr_deformable_decoder=False, dabdetr_deformable_encoder=False, dabdetr_yolo_like_anchor_update=False, data_aug_max_size=1333, data_aug_scale_overlap=None, data_aug_scales=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], data_aug_scales2_crop=[384, 600], data_aug_scales2_resize=[400, 500, 600], dataset_file='coco', ddetr_lr_param=False, debug=False, dec_layer_number=None, dec_layers=6, dec_n_points=4, dec_pred_bbox_embed_share=True, dec_pred_class_embed_share=True, decoder_layer_noise=False, decoder_module_seq=['sa', 'ca', 'ffn'], decoder_sa_type='sa', device='cuda', dice_loss_coef=1.0, dilation=False, dim_feedforward=2048, dist_url='env://', distributed=False, dln_hw_noise=0.2, dln_xy_noise=0.2, dn_box_noise_scale=0.4, dn_label_noise_ratio=0.5, dn_labelbook_size=25, dn_number=100, dropout=0.0, ema_decay=0.9997, ema_epoch=0, embed_init_tgt=True, enc_layers=6, enc_loss_coef=1.0, enc_n_points=4, epochs=20, eval=False, find_unused_params=False, finetune_ignore=None, fix_refpoints_hw=-1, fix_size=False, focal_alpha=0.25, frozen_weights=None, gaze_hidden_size=256, giou_loss_coef=2.0, head_backbone='resnet50', hidden_dim=256, interm_loss_coef=1.0, local_rank=0, lr=0.0001, lr_backbone=1e-05, lr_backbone_names=['backbone.0'], lr_drop=11, lr_drop_list=[33, 45], lr_linear_proj_mult=0.1, lr_linear_proj_names=['reference_points', 'sampling_offsets'], mask_loss_coef=1.0, masks=False, match_unstable_error=True, matcher_type='HungarianMatcher', modelname='TransGOP', multi_step_lr=False, nheads=8, nms_iou_threshold=-1, no_interm_box_loss=False, note='', num_classes=25, num_feature_levels=4, num_patterns=0, num_queries=200, num_select=300, num_workers=0, onecyclelr=False, options=None, output_dir='logs/', param_dict_type='default', pdetr3_bbox_embed_diff_each_layer=False, pdetr3_refHW=-1, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, query_dim=4, random_refpoints_xy=False, rank=0, remove_difficult=False, resume='', return_interm_indices=[1, 2, 3], save_checkpoint_interval=1, save_log=False, save_results=False, seed=42, set_cost_bbox=5.0, set_cost_class=2.0, set_cost_giou=2.0, start_epoch=0, test=False, transformer_activation='relu', two_stage_add_query_num=0, two_stage_bbox_embed_share=False, two_stage_class_embed_share=False, two_stage_default_hw=0.05, two_stage_keep_all_tokens=False, two_stage_learn_wh=False, two_stage_pat_embed=0, two_stage_type='standard', unic_layers=0, use_checkpoint=False, use_deformable_box_attn=False, use_detached_boxes_dec_out=False, use_dn=True, use_ema=False, weight_decay=0.0001, world_size=1)

[04/26 10:28:12.607]: git:
  sha: 2416279a9e010639b5e1f6922b96454707e62207, status: has uncommited changes, branch: main

[04/26 10:28:12.608]: Command: /data1/gcx003/Transgop_list/TransGOP/main.py -c config/TransGOP/TransGOP_4scale.py
[04/26 10:28:12.608]: Full config saved to logs/config_args_all.json
[04/26 10:28:12.608]: world size: 1
[04/26 10:28:12.608]: rank: 0
[04/26 10:28:12.608]: local_rank: 0
[04/26 10:28:12.608]: args: Namespace(add_channel_attention=False, add_pos_value=False, amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=8, bbox_loss_coef=5.0, box_attn_type='roi_align', clip_max_norm=0.1, cls_loss_coef=1.0, coco_panoptic_path=None, coco_path='/data1/gcx003/Datasets/gooreal/gooreal_gazedino/', config_file='config/TransGOP/TransGOP_4scale.py', dabdetr_deformable_decoder=False, dabdetr_deformable_encoder=False, dabdetr_yolo_like_anchor_update=False, data_aug_max_size=1333, data_aug_scale_overlap=None, data_aug_scales=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], data_aug_scales2_crop=[384, 600], data_aug_scales2_resize=[400, 500, 600], dataset_file='coco', ddetr_lr_param=False, debug=False, dec_layer_number=None, dec_layers=6, dec_n_points=4, dec_pred_bbox_embed_share=True, dec_pred_class_embed_share=True, decoder_layer_noise=False, decoder_module_seq=['sa', 'ca', 'ffn'], decoder_sa_type='sa', device='cuda', dice_loss_coef=1.0, dilation=False, dim_feedforward=2048, dist_url='env://', distributed=False, dln_hw_noise=0.2, dln_xy_noise=0.2, dn_box_noise_scale=0.4, dn_label_noise_ratio=0.5, dn_labelbook_size=25, dn_number=100, dropout=0.0, ema_decay=0.9997, ema_epoch=0, embed_init_tgt=True, enc_layers=6, enc_loss_coef=1.0, enc_n_points=4, epochs=20, eval=False, find_unused_params=False, finetune_ignore=None, fix_refpoints_hw=-1, fix_size=False, focal_alpha=0.25, frozen_weights=None, gaze_hidden_size=256, giou_loss_coef=2.0, head_backbone='resnet50', hidden_dim=256, interm_loss_coef=1.0, local_rank=0, lr=0.0001, lr_backbone=1e-05, lr_backbone_names=['backbone.0'], lr_drop=11, lr_drop_list=[33, 45], lr_linear_proj_mult=0.1, lr_linear_proj_names=['reference_points', 'sampling_offsets'], mask_loss_coef=1.0, masks=False, match_unstable_error=True, matcher_type='HungarianMatcher', modelname='TransGOP', multi_step_lr=False, nheads=8, nms_iou_threshold=-1, no_interm_box_loss=False, note='', num_classes=25, num_feature_levels=4, num_patterns=0, num_queries=200, num_select=300, num_workers=0, onecyclelr=False, options=None, output_dir='logs/', param_dict_type='default', pdetr3_bbox_embed_diff_each_layer=False, pdetr3_refHW=-1, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, query_dim=4, random_refpoints_xy=False, rank=0, remove_difficult=False, resume='', return_interm_indices=[1, 2, 3], save_checkpoint_interval=1, save_log=False, save_results=False, seed=42, set_cost_bbox=5.0, set_cost_class=2.0, set_cost_giou=2.0, start_epoch=0, test=False, transformer_activation='relu', two_stage_add_query_num=0, two_stage_bbox_embed_share=False, two_stage_class_embed_share=False, two_stage_default_hw=0.05, two_stage_keep_all_tokens=False, two_stage_learn_wh=False, two_stage_pat_embed=0, two_stage_type='standard', unic_layers=0, use_checkpoint=False, use_deformable_box_attn=False, use_detached_boxes_dec_out=False, use_dn=True, use_ema=False, weight_decay=0.0001, world_size=1)

[04/26 10:28:14.545]: number of params:94285716
[04/26 10:28:14.547]: params:
{
  "transformer.level_embed": 1024,
  "transformer.encoder.layers.0.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.0.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.0.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.0.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.0.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.0.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.0.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.0.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.0.norm1.weight": 256,
  "transformer.encoder.layers.0.norm1.bias": 256,
  "transformer.encoder.layers.0.linear1.weight": 524288,
  "transformer.encoder.layers.0.linear1.bias": 2048,
  "transformer.encoder.layers.0.linear2.weight": 524288,
  "transformer.encoder.layers.0.linear2.bias": 256,
  "transformer.encoder.layers.0.norm2.weight": 256,
  "transformer.encoder.layers.0.norm2.bias": 256,
  "transformer.encoder.layers.1.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.1.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.1.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.1.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.1.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.1.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.1.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.1.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.1.norm1.weight": 256,
  "transformer.encoder.layers.1.norm1.bias": 256,
  "transformer.encoder.layers.1.linear1.weight": 524288,
  "transformer.encoder.layers.1.linear1.bias": 2048,
  "transformer.encoder.layers.1.linear2.weight": 524288,
  "transformer.encoder.layers.1.linear2.bias": 256,
  "transformer.encoder.layers.1.norm2.weight": 256,
  "transformer.encoder.layers.1.norm2.bias": 256,
  "transformer.encoder.layers.2.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.2.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.2.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.2.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.2.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.2.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.2.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.2.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.2.norm1.weight": 256,
  "transformer.encoder.layers.2.norm1.bias": 256,
  "transformer.encoder.layers.2.linear1.weight": 524288,
  "transformer.encoder.layers.2.linear1.bias": 2048,
  "transformer.encoder.layers.2.linear2.weight": 524288,
  "transformer.encoder.layers.2.linear2.bias": 256,
  "transformer.encoder.layers.2.norm2.weight": 256,
  "transformer.encoder.layers.2.norm2.bias": 256,
  "transformer.encoder.layers.3.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.3.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.3.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.3.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.3.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.3.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.3.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.3.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.3.norm1.weight": 256,
  "transformer.encoder.layers.3.norm1.bias": 256,
  "transformer.encoder.layers.3.linear1.weight": 524288,
  "transformer.encoder.layers.3.linear1.bias": 2048,
  "transformer.encoder.layers.3.linear2.weight": 524288,
  "transformer.encoder.layers.3.linear2.bias": 256,
  "transformer.encoder.layers.3.norm2.weight": 256,
  "transformer.encoder.layers.3.norm2.bias": 256,
  "transformer.encoder.layers.4.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.4.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.4.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.4.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.4.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.4.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.4.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.4.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.4.norm1.weight": 256,
  "transformer.encoder.layers.4.norm1.bias": 256,
  "transformer.encoder.layers.4.linear1.weight": 524288,
  "transformer.encoder.layers.4.linear1.bias": 2048,
  "transformer.encoder.layers.4.linear2.weight": 524288,
  "transformer.encoder.layers.4.linear2.bias": 256,
  "transformer.encoder.layers.4.norm2.weight": 256,
  "transformer.encoder.layers.4.norm2.bias": 256,
  "transformer.encoder.layers.5.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.5.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.5.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.5.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.5.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.5.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.5.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.5.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.5.norm1.weight": 256,
  "transformer.encoder.layers.5.norm1.bias": 256,
  "transformer.encoder.layers.5.linear1.weight": 524288,
  "transformer.encoder.layers.5.linear1.bias": 2048,
  "transformer.encoder.layers.5.linear2.weight": 524288,
  "transformer.encoder.layers.5.linear2.bias": 256,
  "transformer.encoder.layers.5.norm2.weight": 256,
  "transformer.encoder.layers.5.norm2.bias": 256,
  "transformer.decoder.layers.0.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.0.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.0.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.0.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.0.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.0.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.0.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.0.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.0.norm1.weight": 256,
  "transformer.decoder.layers.0.norm1.bias": 256,
  "transformer.decoder.layers.0.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.0.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.0.norm2.weight": 256,
  "transformer.decoder.layers.0.norm2.bias": 256,
  "transformer.decoder.layers.0.linear1.weight": 524288,
  "transformer.decoder.layers.0.linear1.bias": 2048,
  "transformer.decoder.layers.0.linear2.weight": 524288,
  "transformer.decoder.layers.0.linear2.bias": 256,
  "transformer.decoder.layers.0.norm3.weight": 256,
  "transformer.decoder.layers.0.norm3.bias": 256,
  "transformer.decoder.layers.1.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.1.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.1.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.1.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.1.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.1.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.1.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.1.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.1.norm1.weight": 256,
  "transformer.decoder.layers.1.norm1.bias": 256,
  "transformer.decoder.layers.1.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.1.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.1.norm2.weight": 256,
  "transformer.decoder.layers.1.norm2.bias": 256,
  "transformer.decoder.layers.1.linear1.weight": 524288,
  "transformer.decoder.layers.1.linear1.bias": 2048,
  "transformer.decoder.layers.1.linear2.weight": 524288,
  "transformer.decoder.layers.1.linear2.bias": 256,
  "transformer.decoder.layers.1.norm3.weight": 256,
  "transformer.decoder.layers.1.norm3.bias": 256,
  "transformer.decoder.layers.2.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.2.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.2.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.2.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.2.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.2.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.2.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.2.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.2.norm1.weight": 256,
  "transformer.decoder.layers.2.norm1.bias": 256,
  "transformer.decoder.layers.2.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.2.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.2.norm2.weight": 256,
  "transformer.decoder.layers.2.norm2.bias": 256,
  "transformer.decoder.layers.2.linear1.weight": 524288,
  "transformer.decoder.layers.2.linear1.bias": 2048,
  "transformer.decoder.layers.2.linear2.weight": 524288,
  "transformer.decoder.layers.2.linear2.bias": 256,
  "transformer.decoder.layers.2.norm3.weight": 256,
  "transformer.decoder.layers.2.norm3.bias": 256,
  "transformer.decoder.layers.3.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.3.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.3.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.3.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.3.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.3.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.3.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.3.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.3.norm1.weight": 256,
  "transformer.decoder.layers.3.norm1.bias": 256,
  "transformer.decoder.layers.3.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.3.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.3.norm2.weight": 256,
  "transformer.decoder.layers.3.norm2.bias": 256,
  "transformer.decoder.layers.3.linear1.weight": 524288,
  "transformer.decoder.layers.3.linear1.bias": 2048,
  "transformer.decoder.layers.3.linear2.weight": 524288,
  "transformer.decoder.layers.3.linear2.bias": 256,
  "transformer.decoder.layers.3.norm3.weight": 256,
  "transformer.decoder.layers.3.norm3.bias": 256,
  "transformer.decoder.layers.4.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.4.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.4.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.4.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.4.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.4.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.4.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.4.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.4.norm1.weight": 256,
  "transformer.decoder.layers.4.norm1.bias": 256,
  "transformer.decoder.layers.4.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.4.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.4.norm2.weight": 256,
  "transformer.decoder.layers.4.norm2.bias": 256,
  "transformer.decoder.layers.4.linear1.weight": 524288,
  "transformer.decoder.layers.4.linear1.bias": 2048,
  "transformer.decoder.layers.4.linear2.weight": 524288,
  "transformer.decoder.layers.4.linear2.bias": 256,
  "transformer.decoder.layers.4.norm3.weight": 256,
  "transformer.decoder.layers.4.norm3.bias": 256,
  "transformer.decoder.layers.5.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.5.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.5.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.5.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.5.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.5.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.5.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.5.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.5.norm1.weight": 256,
  "transformer.decoder.layers.5.norm1.bias": 256,
  "transformer.decoder.layers.5.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.5.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.5.norm2.weight": 256,
  "transformer.decoder.layers.5.norm2.bias": 256,
  "transformer.decoder.layers.5.linear1.weight": 524288,
  "transformer.decoder.layers.5.linear1.bias": 2048,
  "transformer.decoder.layers.5.linear2.weight": 524288,
  "transformer.decoder.layers.5.linear2.bias": 256,
  "transformer.decoder.layers.5.norm3.weight": 256,
  "transformer.decoder.layers.5.norm3.bias": 256,
  "transformer.decoder.norm.weight": 256,
  "transformer.decoder.norm.bias": 256,
  "transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "transformer.decoder.ref_point_head.layers.0.bias": 256,
  "transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "transformer.decoder.ref_point_head.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.0.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.0.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.0.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.0.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.0.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.0.layers.2.bias": 4,
  "transformer.decoder.class_embed.0.weight": 6400,
  "transformer.decoder.class_embed.0.bias": 25,
  "transformer.tgt_embed.weight": 51200,
  "transformer.enc_output.weight": 65536,
  "transformer.enc_output.bias": 256,
  "transformer.enc_output_norm.weight": 256,
  "transformer.enc_output_norm.bias": 256,
  "transformer.enc_out_bbox_embed.layers.0.weight": 65536,
  "transformer.enc_out_bbox_embed.layers.0.bias": 256,
  "transformer.enc_out_bbox_embed.layers.1.weight": 65536,
  "transformer.enc_out_bbox_embed.layers.1.bias": 256,
  "transformer.enc_out_bbox_embed.layers.2.weight": 1024,
  "transformer.enc_out_bbox_embed.layers.2.bias": 4,
  "transformer.enc_out_class_embed.weight": 6400,
  "transformer.enc_out_class_embed.bias": 25,
  "gatector.backbone.model.conv2.weight": 9408,
  "gatector.backbone.model.bn2.weight": 64,
  "gatector.backbone.model.bn2.bias": 64,
  "gatector.backbone.model.layer1.0.conv1.weight": 4096,
  "gatector.backbone.model.layer1.0.bn1.weight": 64,
  "gatector.backbone.model.layer1.0.bn1.bias": 64,
  "gatector.backbone.model.layer1.0.conv2.weight": 36864,
  "gatector.backbone.model.layer1.0.bn2.weight": 64,
  "gatector.backbone.model.layer1.0.bn2.bias": 64,
  "gatector.backbone.model.layer1.0.conv3.weight": 16384,
  "gatector.backbone.model.layer1.0.bn3.weight": 256,
  "gatector.backbone.model.layer1.0.bn3.bias": 256,
  "gatector.backbone.model.layer1.0.downsample.0.weight": 16384,
  "gatector.backbone.model.layer1.0.downsample.1.weight": 256,
  "gatector.backbone.model.layer1.0.downsample.1.bias": 256,
  "gatector.backbone.model.layer1.1.conv1.weight": 16384,
  "gatector.backbone.model.layer1.1.bn1.weight": 64,
  "gatector.backbone.model.layer1.1.bn1.bias": 64,
  "gatector.backbone.model.layer1.1.conv2.weight": 36864,
  "gatector.backbone.model.layer1.1.bn2.weight": 64,
  "gatector.backbone.model.layer1.1.bn2.bias": 64,
  "gatector.backbone.model.layer1.1.conv3.weight": 16384,
  "gatector.backbone.model.layer1.1.bn3.weight": 256,
  "gatector.backbone.model.layer1.1.bn3.bias": 256,
  "gatector.backbone.model.layer1.2.conv1.weight": 16384,
  "gatector.backbone.model.layer1.2.bn1.weight": 64,
  "gatector.backbone.model.layer1.2.bn1.bias": 64,
  "gatector.backbone.model.layer1.2.conv2.weight": 36864,
  "gatector.backbone.model.layer1.2.bn2.weight": 64,
  "gatector.backbone.model.layer1.2.bn2.bias": 64,
  "gatector.backbone.model.layer1.2.conv3.weight": 16384,
  "gatector.backbone.model.layer1.2.bn3.weight": 256,
  "gatector.backbone.model.layer1.2.bn3.bias": 256,
  "gatector.backbone.model.layer2.0.conv1.weight": 32768,
  "gatector.backbone.model.layer2.0.bn1.weight": 128,
  "gatector.backbone.model.layer2.0.bn1.bias": 128,
  "gatector.backbone.model.layer2.0.conv2.weight": 147456,
  "gatector.backbone.model.layer2.0.bn2.weight": 128,
  "gatector.backbone.model.layer2.0.bn2.bias": 128,
  "gatector.backbone.model.layer2.0.conv3.weight": 65536,
  "gatector.backbone.model.layer2.0.bn3.weight": 512,
  "gatector.backbone.model.layer2.0.bn3.bias": 512,
  "gatector.backbone.model.layer2.0.downsample.0.weight": 131072,
  "gatector.backbone.model.layer2.0.downsample.1.weight": 512,
  "gatector.backbone.model.layer2.0.downsample.1.bias": 512,
  "gatector.backbone.model.layer2.1.conv1.weight": 65536,
  "gatector.backbone.model.layer2.1.bn1.weight": 128,
  "gatector.backbone.model.layer2.1.bn1.bias": 128,
  "gatector.backbone.model.layer2.1.conv2.weight": 147456,
  "gatector.backbone.model.layer2.1.bn2.weight": 128,
  "gatector.backbone.model.layer2.1.bn2.bias": 128,
  "gatector.backbone.model.layer2.1.conv3.weight": 65536,
  "gatector.backbone.model.layer2.1.bn3.weight": 512,
  "gatector.backbone.model.layer2.1.bn3.bias": 512,
  "gatector.backbone.model.layer2.2.conv1.weight": 65536,
  "gatector.backbone.model.layer2.2.bn1.weight": 128,
  "gatector.backbone.model.layer2.2.bn1.bias": 128,
  "gatector.backbone.model.layer2.2.conv2.weight": 147456,
  "gatector.backbone.model.layer2.2.bn2.weight": 128,
  "gatector.backbone.model.layer2.2.bn2.bias": 128,
  "gatector.backbone.model.layer2.2.conv3.weight": 65536,
  "gatector.backbone.model.layer2.2.bn3.weight": 512,
  "gatector.backbone.model.layer2.2.bn3.bias": 512,
  "gatector.backbone.model.layer2.3.conv1.weight": 65536,
  "gatector.backbone.model.layer2.3.bn1.weight": 128,
  "gatector.backbone.model.layer2.3.bn1.bias": 128,
  "gatector.backbone.model.layer2.3.conv2.weight": 147456,
  "gatector.backbone.model.layer2.3.bn2.weight": 128,
  "gatector.backbone.model.layer2.3.bn2.bias": 128,
  "gatector.backbone.model.layer2.3.conv3.weight": 65536,
  "gatector.backbone.model.layer2.3.bn3.weight": 512,
  "gatector.backbone.model.layer2.3.bn3.bias": 512,
  "gatector.backbone.model.layer3.0.conv1.weight": 131072,
  "gatector.backbone.model.layer3.0.bn1.weight": 256,
  "gatector.backbone.model.layer3.0.bn1.bias": 256,
  "gatector.backbone.model.layer3.0.conv2.weight": 589824,
  "gatector.backbone.model.layer3.0.bn2.weight": 256,
  "gatector.backbone.model.layer3.0.bn2.bias": 256,
  "gatector.backbone.model.layer3.0.conv3.weight": 262144,
  "gatector.backbone.model.layer3.0.bn3.weight": 1024,
  "gatector.backbone.model.layer3.0.bn3.bias": 1024,
  "gatector.backbone.model.layer3.0.downsample.0.weight": 524288,
  "gatector.backbone.model.layer3.0.downsample.1.weight": 1024,
  "gatector.backbone.model.layer3.0.downsample.1.bias": 1024,
  "gatector.backbone.model.layer3.1.conv1.weight": 262144,
  "gatector.backbone.model.layer3.1.bn1.weight": 256,
  "gatector.backbone.model.layer3.1.bn1.bias": 256,
  "gatector.backbone.model.layer3.1.conv2.weight": 589824,
  "gatector.backbone.model.layer3.1.bn2.weight": 256,
  "gatector.backbone.model.layer3.1.bn2.bias": 256,
  "gatector.backbone.model.layer3.1.conv3.weight": 262144,
  "gatector.backbone.model.layer3.1.bn3.weight": 1024,
  "gatector.backbone.model.layer3.1.bn3.bias": 1024,
  "gatector.backbone.model.layer3.2.conv1.weight": 262144,
  "gatector.backbone.model.layer3.2.bn1.weight": 256,
  "gatector.backbone.model.layer3.2.bn1.bias": 256,
  "gatector.backbone.model.layer3.2.conv2.weight": 589824,
  "gatector.backbone.model.layer3.2.bn2.weight": 256,
  "gatector.backbone.model.layer3.2.bn2.bias": 256,
  "gatector.backbone.model.layer3.2.conv3.weight": 262144,
  "gatector.backbone.model.layer3.2.bn3.weight": 1024,
  "gatector.backbone.model.layer3.2.bn3.bias": 1024,
  "gatector.backbone.model.layer3.3.conv1.weight": 262144,
  "gatector.backbone.model.layer3.3.bn1.weight": 256,
  "gatector.backbone.model.layer3.3.bn1.bias": 256,
  "gatector.backbone.model.layer3.3.conv2.weight": 589824,
  "gatector.backbone.model.layer3.3.bn2.weight": 256,
  "gatector.backbone.model.layer3.3.bn2.bias": 256,
  "gatector.backbone.model.layer3.3.conv3.weight": 262144,
  "gatector.backbone.model.layer3.3.bn3.weight": 1024,
  "gatector.backbone.model.layer3.3.bn3.bias": 1024,
  "gatector.backbone.model.layer3.4.conv1.weight": 262144,
  "gatector.backbone.model.layer3.4.bn1.weight": 256,
  "gatector.backbone.model.layer3.4.bn1.bias": 256,
  "gatector.backbone.model.layer3.4.conv2.weight": 589824,
  "gatector.backbone.model.layer3.4.bn2.weight": 256,
  "gatector.backbone.model.layer3.4.bn2.bias": 256,
  "gatector.backbone.model.layer3.4.conv3.weight": 262144,
  "gatector.backbone.model.layer3.4.bn3.weight": 1024,
  "gatector.backbone.model.layer3.4.bn3.bias": 1024,
  "gatector.backbone.model.layer3.5.conv1.weight": 262144,
  "gatector.backbone.model.layer3.5.bn1.weight": 256,
  "gatector.backbone.model.layer3.5.bn1.bias": 256,
  "gatector.backbone.model.layer3.5.conv2.weight": 589824,
  "gatector.backbone.model.layer3.5.bn2.weight": 256,
  "gatector.backbone.model.layer3.5.bn2.bias": 256,
  "gatector.backbone.model.layer3.5.conv3.weight": 262144,
  "gatector.backbone.model.layer3.5.bn3.weight": 1024,
  "gatector.backbone.model.layer3.5.bn3.bias": 1024,
  "gatector.backbone.model.layer4.0.conv1.weight": 524288,
  "gatector.backbone.model.layer4.0.bn1.weight": 512,
  "gatector.backbone.model.layer4.0.bn1.bias": 512,
  "gatector.backbone.model.layer4.0.conv2.weight": 2359296,
  "gatector.backbone.model.layer4.0.bn2.weight": 512,
  "gatector.backbone.model.layer4.0.bn2.bias": 512,
  "gatector.backbone.model.layer4.0.conv3.weight": 1048576,
  "gatector.backbone.model.layer4.0.bn3.weight": 2048,
  "gatector.backbone.model.layer4.0.bn3.bias": 2048,
  "gatector.backbone.model.layer4.0.downsample.0.weight": 2097152,
  "gatector.backbone.model.layer4.0.downsample.1.weight": 2048,
  "gatector.backbone.model.layer4.0.downsample.1.bias": 2048,
  "gatector.backbone.model.layer4.1.conv1.weight": 1048576,
  "gatector.backbone.model.layer4.1.bn1.weight": 512,
  "gatector.backbone.model.layer4.1.bn1.bias": 512,
  "gatector.backbone.model.layer4.1.conv2.weight": 2359296,
  "gatector.backbone.model.layer4.1.bn2.weight": 512,
  "gatector.backbone.model.layer4.1.bn2.bias": 512,
  "gatector.backbone.model.layer4.1.conv3.weight": 1048576,
  "gatector.backbone.model.layer4.1.bn3.weight": 2048,
  "gatector.backbone.model.layer4.1.bn3.bias": 2048,
  "gatector.backbone.model.layer4.2.conv1.weight": 1048576,
  "gatector.backbone.model.layer4.2.bn1.weight": 512,
  "gatector.backbone.model.layer4.2.bn1.bias": 512,
  "gatector.backbone.model.layer4.2.conv2.weight": 2359296,
  "gatector.backbone.model.layer4.2.bn2.weight": 512,
  "gatector.backbone.model.layer4.2.bn2.bias": 512,
  "gatector.backbone.model.layer4.2.conv3.weight": 1048576,
  "gatector.backbone.model.layer4.2.bn3.weight": 2048,
  "gatector.backbone.model.layer4.2.bn3.bias": 2048,
  "gatector.backbone.model.layer5_scene.0.conv1.weight": 524288,
  "gatector.backbone.model.layer5_scene.0.bn1.weight": 256,
  "gatector.backbone.model.layer5_scene.0.bn1.bias": 256,
  "gatector.backbone.model.layer5_scene.0.conv2.weight": 589824,
  "gatector.backbone.model.layer5_scene.0.bn2.weight": 256,
  "gatector.backbone.model.layer5_scene.0.bn2.bias": 256,
  "gatector.backbone.model.layer5_scene.0.conv3.weight": 262144,
  "gatector.backbone.model.layer5_scene.0.bn3.weight": 1024,
  "gatector.backbone.model.layer5_scene.0.bn3.bias": 1024,
  "gatector.backbone.model.layer5_scene.0.downsample.0.weight": 2097152,
  "gatector.backbone.model.layer5_scene.0.downsample.1.weight": 1024,
  "gatector.backbone.model.layer5_scene.0.downsample.1.bias": 1024,
  "gatector.backbone.model.layer5_scene.1.conv1.weight": 262144,
  "gatector.backbone.model.layer5_scene.1.bn1.weight": 256,
  "gatector.backbone.model.layer5_scene.1.bn1.bias": 256,
  "gatector.backbone.model.layer5_scene.1.conv2.weight": 589824,
  "gatector.backbone.model.layer5_scene.1.bn2.weight": 256,
  "gatector.backbone.model.layer5_scene.1.bn2.bias": 256,
  "gatector.backbone.model.layer5_scene.1.conv3.weight": 262144,
  "gatector.backbone.model.layer5_scene.1.bn3.weight": 1024,
  "gatector.backbone.model.layer5_scene.1.bn3.bias": 1024,
  "gatector.backbone.model.layer5_face.0.conv1.weight": 524288,
  "gatector.backbone.model.layer5_face.0.bn1.weight": 256,
  "gatector.backbone.model.layer5_face.0.bn1.bias": 256,
  "gatector.backbone.model.layer5_face.0.conv2.weight": 589824,
  "gatector.backbone.model.layer5_face.0.bn2.weight": 256,
  "gatector.backbone.model.layer5_face.0.bn2.bias": 256,
  "gatector.backbone.model.layer5_face.0.conv3.weight": 262144,
  "gatector.backbone.model.layer5_face.0.bn3.weight": 1024,
  "gatector.backbone.model.layer5_face.0.bn3.bias": 1024,
  "gatector.backbone.model.layer5_face.0.downsample.0.weight": 2097152,
  "gatector.backbone.model.layer5_face.0.downsample.1.weight": 1024,
  "gatector.backbone.model.layer5_face.0.downsample.1.bias": 1024,
  "gatector.backbone.model.layer5_face.1.conv1.weight": 262144,
  "gatector.backbone.model.layer5_face.1.bn1.weight": 256,
  "gatector.backbone.model.layer5_face.1.bn1.bias": 256,
  "gatector.backbone.model.layer5_face.1.conv2.weight": 589824,
  "gatector.backbone.model.layer5_face.1.bn2.weight": 256,
  "gatector.backbone.model.layer5_face.1.bn2.bias": 256,
  "gatector.backbone.model.layer5_face.1.conv3.weight": 262144,
  "gatector.backbone.model.layer5_face.1.bn3.weight": 1024,
  "gatector.backbone.model.layer5_face.1.bn3.bias": 1024,
  "gatector.conv_face_scene.weight": 5242880,
  "gatector.conv_face_scene.bias": 2048,
  "gatector.conv_trblock.weight": 65536,
  "gatector.conv_trblock.bias": 256,
  "gatector.trblock_bn.weight": 256,
  "gatector.trblock_bn.bias": 256,
  "gatector.conv_block.0.weight": 288,
  "gatector.conv_block.2.weight": 18432,
  "gatector.conv_block.4.weight": 73728,
  "gatector.conv_block.6.weight": 294912,
  "gatector.conv_block.8.weight": 1179648,
  "gatector.attn.weight": 88592,
  "gatector.attn.bias": 49,
  "gatector.compress_conv1.weight": 2097152,
  "gatector.compress_bn1.weight": 1024,
  "gatector.compress_bn1.bias": 1024,
  "gatector.compress_conv2.weight": 524288,
  "gatector.compress_bn2.weight": 512,
  "gatector.compress_bn2.bias": 512,
  "gatector.compress_conv1_inout.weight": 1048576,
  "gatector.compress_bn1_inout.weight": 512,
  "gatector.compress_bn1_inout.bias": 512,
  "gatector.compress_conv2_inout.weight": 512,
  "gatector.compress_bn2_inout.weight": 1,
  "gatector.compress_bn2_inout.bias": 1,
  "gatector.fc_inout.weight": 49,
  "gatector.fc_inout.bias": 1,
  "gatector.deconv1.weight": 1179648,
  "gatector.deconv1.bias": 256,
  "gatector.deconv_bn1.weight": 256,
  "gatector.deconv_bn1.bias": 256,
  "gatector.deconv2.weight": 294912,
  "gatector.deconv2.bias": 128,
  "gatector.deconv_bn2.weight": 128,
  "gatector.deconv_bn2.bias": 128,
  "gatector.deconv3.weight": 2048,
  "gatector.deconv3.bias": 1,
  "gatector.deconv_bn3.weight": 1,
  "gatector.deconv_bn3.bias": 1,
  "gatector.conv4.weight": 1,
  "gatector.conv4.bias": 1,
  "gatector.totrans_conv.weight": 65536,
  "gatector.totrans_conv_bn1.weight": 256,
  "gatector.totrans_conv_bn1.bias": 256,
  "gatector.transformer_layer.encoder.layers.0.self_attn.in_proj_weight": 196608,
  "gatector.transformer_layer.encoder.layers.0.self_attn.in_proj_bias": 768,
  "gatector.transformer_layer.encoder.layers.0.self_attn.out_proj.weight": 65536,
  "gatector.transformer_layer.encoder.layers.0.self_attn.out_proj.bias": 256,
  "gatector.transformer_layer.encoder.layers.0.linear1.weight": 524288,
  "gatector.transformer_layer.encoder.layers.0.linear1.bias": 2048,
  "gatector.transformer_layer.encoder.layers.0.linear2.weight": 524288,
  "gatector.transformer_layer.encoder.layers.0.linear2.bias": 256,
  "gatector.transformer_layer.encoder.layers.0.norm1.weight": 256,
  "gatector.transformer_layer.encoder.layers.0.norm1.bias": 256,
  "gatector.transformer_layer.encoder.layers.0.norm2.weight": 256,
  "gatector.transformer_layer.encoder.layers.0.norm2.bias": 256,
  "gatector.transformer_layer.decoder.layers.0.self_attn.in_proj_weight": 196608,
  "gatector.transformer_layer.decoder.layers.0.self_attn.in_proj_bias": 768,
  "gatector.transformer_layer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "gatector.transformer_layer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "gatector.transformer_layer.decoder.layers.0.multihead_attn.in_proj_weight": 196608,
  "gatector.transformer_layer.decoder.layers.0.multihead_attn.in_proj_bias": 768,
  "gatector.transformer_layer.decoder.layers.0.multihead_attn.out_proj.weight": 65536,
  "gatector.transformer_layer.decoder.layers.0.multihead_attn.out_proj.bias": 256,
  "gatector.transformer_layer.decoder.layers.0.linear1.weight": 524288,
  "gatector.transformer_layer.decoder.layers.0.linear1.bias": 2048,
  "gatector.transformer_layer.decoder.layers.0.linear2.weight": 524288,
  "gatector.transformer_layer.decoder.layers.0.linear2.bias": 256,
  "gatector.transformer_layer.decoder.layers.0.norm1.weight": 256,
  "gatector.transformer_layer.decoder.layers.0.norm1.bias": 256,
  "gatector.transformer_layer.decoder.layers.0.norm2.weight": 256,
  "gatector.transformer_layer.decoder.layers.0.norm2.bias": 256,
  "gatector.transformer_layer.decoder.layers.0.norm3.weight": 256,
  "gatector.transformer_layer.decoder.layers.0.norm3.bias": 256,
  "gatector.transformer_layer.decoder.layers.0.fc_to512.weight": 65536,
  "gatector.transformer_layer.decoder.layers.0.fc_to512.bias": 256,
  "gatector.transformer_layer.decoder.norm.weight": 256,
  "gatector.transformer_layer.decoder.norm.bias": 256,
  "label_enc.weight": 6656,
  "input_proj.0.0.weight": 131072,
  "input_proj.0.0.bias": 256,
  "input_proj.0.1.weight": 256,
  "input_proj.0.1.bias": 256,
  "input_proj.1.0.weight": 262144,
  "input_proj.1.0.bias": 256,
  "input_proj.1.1.weight": 256,
  "input_proj.1.1.bias": 256,
  "input_proj.2.0.weight": 524288,
  "input_proj.2.0.bias": 256,
  "input_proj.2.1.weight": 256,
  "input_proj.2.1.bias": 256,
  "input_proj.3.0.weight": 4718592,
  "input_proj.3.0.bias": 256,
  "input_proj.3.1.weight": 256,
  "input_proj.3.1.bias": 256,
  "backbone.0.body.layer2.0.conv1.weight": 32768,
  "backbone.0.body.layer2.0.conv2.weight": 147456,
  "backbone.0.body.layer2.0.conv3.weight": 65536,
  "backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "backbone.0.body.layer2.1.conv1.weight": 65536,
  "backbone.0.body.layer2.1.conv2.weight": 147456,
  "backbone.0.body.layer2.1.conv3.weight": 65536,
  "backbone.0.body.layer2.2.conv1.weight": 65536,
  "backbone.0.body.layer2.2.conv2.weight": 147456,
  "backbone.0.body.layer2.2.conv3.weight": 65536,
  "backbone.0.body.layer2.3.conv1.weight": 65536,
  "backbone.0.body.layer2.3.conv2.weight": 147456,
  "backbone.0.body.layer2.3.conv3.weight": 65536,
  "backbone.0.body.layer3.0.conv1.weight": 131072,
  "backbone.0.body.layer3.0.conv2.weight": 589824,
  "backbone.0.body.layer3.0.conv3.weight": 262144,
  "backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "backbone.0.body.layer3.1.conv1.weight": 262144,
  "backbone.0.body.layer3.1.conv2.weight": 589824,
  "backbone.0.body.layer3.1.conv3.weight": 262144,
  "backbone.0.body.layer3.2.conv1.weight": 262144,
  "backbone.0.body.layer3.2.conv2.weight": 589824,
  "backbone.0.body.layer3.2.conv3.weight": 262144,
  "backbone.0.body.layer3.3.conv1.weight": 262144,
  "backbone.0.body.layer3.3.conv2.weight": 589824,
  "backbone.0.body.layer3.3.conv3.weight": 262144,
  "backbone.0.body.layer3.4.conv1.weight": 262144,
  "backbone.0.body.layer3.4.conv2.weight": 589824,
  "backbone.0.body.layer3.4.conv3.weight": 262144,
  "backbone.0.body.layer3.5.conv1.weight": 262144,
  "backbone.0.body.layer3.5.conv2.weight": 589824,
  "backbone.0.body.layer3.5.conv3.weight": 262144,
  "backbone.0.body.layer4.0.conv1.weight": 524288,
  "backbone.0.body.layer4.0.conv2.weight": 2359296,
  "backbone.0.body.layer4.0.conv3.weight": 1048576,
  "backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "backbone.0.body.layer4.1.conv1.weight": 1048576,
  "backbone.0.body.layer4.1.conv2.weight": 2359296,
  "backbone.0.body.layer4.1.conv3.weight": 1048576,
  "backbone.0.body.layer4.2.conv1.weight": 1048576,
  "backbone.0.body.layer4.2.conv2.weight": 2359296,
  "backbone.0.body.layer4.2.conv3.weight": 1048576
}
